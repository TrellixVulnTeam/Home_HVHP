{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_processing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPENYSSu14eKs5ke3XBm2yL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-Cs36kMG6dj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow import data\n",
        "from keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_purchase_attributes(inputPath):\n",
        "\t# initialize the list of column names in the CSV file and then\n",
        "\t# load it using Pandas\n",
        "\tcols = ['Location', 'Description', 'Cost', 'Category', 'Day', 'Month', 'Year']\n",
        "\tdf = pd.read_csv(inputPath, header=0, names=cols)\n",
        "\t# return the data frame\n",
        "\treturn df"
      ],
      "metadata": {
        "id": "mR-T5Fn4MFpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_purchase_attributes(df, train, test):\n",
        "\t# initialize the column names of the continuous data\n",
        "\tcontinuous = ['Cost']\n",
        "\t# perform min-max scaling each continuous feature column to the range [0, 1]\n",
        "\tcs = MinMaxScaler()\n",
        "\ttrainContinuous = cs.fit_transform(train[continuous])\n",
        "\ttestContinuous = cs.transform(test[continuous])\n",
        "  # initialize the column names of the categorical data\n",
        "\tcategorical = ['Location', 'Day', 'Month', 'Year']\n",
        "  # one-hot encode the categorical data\n",
        "\tonehot = OneHotEncoder(sparse=False).fit(df[categorical])\n",
        "\ttrainCategorical = onehot.transform(train[categorical])\n",
        "\ttestCategorical = onehot.transform(test[categorical])\n",
        "\t# construct our training and testing data points by concatenating\n",
        "\t# the categorical features with the continuous features\n",
        "\ttrainX = np.hstack([trainCategorical, trainContinuous])\n",
        "\ttestX = np.hstack([testCategorical, testContinuous])\n",
        "\t# return the concatenated training and testing data\n",
        "\treturn (trainX, testX)"
      ],
      "metadata": {
        "id": "o1xzIyHwM-Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_purchase_attributes_all(df):\n",
        "\t# initialize the column names of the continuous data\n",
        "\tcontinuous = ['Cost']\n",
        "\t# perform min-max scaling each continuous feature column to the range [0, 1]\n",
        "\tcs = MinMaxScaler()\n",
        "\tcontinuous = cs.fit_transform(df[continuous])\n",
        "  # initialize the column names of the categorical data\n",
        "\tcategorical = ['Location', 'Day', 'Month', 'Year']\n",
        "  # one-hot encode the categorical data\n",
        "\tonehot = OneHotEncoder(sparse=False).fit(df[categorical])\n",
        "\tcategorical = onehot.transform(df[categorical])\n",
        "\t# construct our data points by concatenating\n",
        "\t# the categorical features with the continuous features\n",
        "\tX = np.hstack([categorical, continuous])\n",
        "\t# return the concatenated data\n",
        "\treturn (X)"
      ],
      "metadata": {
        "id": "xX-pCuuD3I5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_purchase_labels(df, train, test):\n",
        "  # initialize the column names of the categorical data\n",
        "\tcategorical = ['Category']\n",
        "  # one-hot encode the categorical data\n",
        "\tonehot = OneHotEncoder(sparse=False).fit(df)\n",
        "\ttrainY = onehot.transform(train)\n",
        "\ttestY = onehot.transform(test)\n",
        "\treturn (trainY, testY)"
      ],
      "metadata": {
        "id": "QyjmW5Pn3KlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocab_index(train):\n",
        "  vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "  text_ds = data.Dataset.from_tensor_slices(train).batch(128)\n",
        "  vectorizer.adapt(text_ds)\n",
        "  voc = vectorizer.get_vocabulary()\n",
        "  word_index = dict(zip(voc, range(len(voc))))\n",
        "  return voc, word_index, vectorizer"
      ],
      "metadata": {
        "id": "pPCM30huPCLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings_index():\n",
        "  path_to_glove_file = os.path.join(\n",
        "      os.path.expanduser(\"~\"), \"/content/glove.6B.100d.txt\"\n",
        "  )\n",
        "\n",
        "  embeddings_index = {}\n",
        "  with open(path_to_glove_file) as f:\n",
        "      for line in f:\n",
        "          word, coefs = line.split(maxsplit=1)\n",
        "          coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "          embeddings_index[word] = coefs\n",
        "  return embeddings_index"
      ],
      "metadata": {
        "id": "06IFDrNdQNMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_matrix(voc, word_index, embeddings_index):\n",
        "  num_tokens = len(voc) + 2\n",
        "  embedding_dim = 100\n",
        "\n",
        "  # Prepare embedding matrix\n",
        "  embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          # Words not found in embedding index will be all-zeros.\n",
        "          # This includes the representation for \"padding\" and \"OOV\"\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  return num_tokens, embedding_dim, embedding_matrix"
      ],
      "metadata": {
        "id": "JgH_7H7bQxrE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}